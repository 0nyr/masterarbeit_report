"""
This script is used to detect all the data structures in the code.
To do so, we rely on the fact that the heap dump byte file contains
the chunks of memory generated by "GLibC-2.28"'s malloc function.

The documentation of the malloc function is available here:
https://sourceware.org/glibc/wiki/MallocInternals

The implementation code of the malloc function is available here:
https://elixir.bootlin.com/glibc/glibc-2.28/source/malloc/malloc.c

The following is a snippet of the documentation of the malloc function,
directly from the source code of the malloc function:

```c
    Chunks of memory are maintained using a `boundary tag' method as
    described in e.g., Knuth or Standish.  (See the paper by Paul
    Wilson ftp://ftp.cs.utexas.edu/pub/garbage/allocsrv.ps for a
    survey of such techniques.)  Sizes of free chunks are stored both
    in the front of each chunk and at the end.  This makes
    consolidating fragmented chunks into bigger chunks very fast.  The
    size fields also hold bits representing whether chunks are free or
    in use.

    An allocated chunk looks like this:

    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of previous chunk, if unallocated (P clear)  |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of chunk, in bytes                     |A|M|P|
      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             User data starts here...                          .
            .                                                               .
            .             (malloc_usable_size() bytes)                      .
            .                                                               |
nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             (size of chunk, but used for application data)    |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of next chunk, in bytes                |A|0|1|
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

    Where "chunk" is the front of the chunk for the purpose of most of
    the malloc code, but "mem" is the pointer that is returned to the
    user.  "Nextchunk" is the beginning of the next contiguous chunk.

    Chunks always begin on even word boundaries, so the mem portion
    (which is returned to the user) is also on an even word boundary, and
    thus at least double-word aligned.

    Free chunks are stored in circular doubly-linked lists, and look like this:

    chunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of previous chunk, if unallocated (P clear)  |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
`m-header:' |             Size of chunk, in bytes                     |A|0|P|
      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Forward pointer to next chunk in list             |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Back pointer to previous chunk in list            |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Unused space (may be 0 bytes long)                .
            .                                                               .
            .                                                               |
nextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    `foot:' |             Size of chunk, in bytes                           |
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
            |             Size of next chunk, in bytes                |A|0|0|
            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
```

The flags A, M and P are used, according to the documentation, as follows:

A (0x04):   Allocated Arena - the main arena uses the application's heap.
            Other arenas use mmap'd heaps. To map a chunk to a heap, you need
            to know which case applies. If this bit is 0, the chunk comes from
            the main arena and the main heap. If this bit is 1, the chunk
            comes from mmap'd memory and the location of the heap can be
            computed from the chunk's address.

M (0x02):   MMap'd chunk - this chunk was allocated with a single call to mmap
            and is not part of a heap at all.

P (0x01):   Previous chunk is in use - if set, the previous chunk is still
            being used by the application, and thus the prev_size field is
            invalid. Note - some chunks, such as those in fastbins (see below)
            will have this bit set despite being free'd by the application.
            This bit really means that the previous chunk should not be
            considered a candidate for coalescing - it's "in use" by either
            the application or some other optimization layered atop malloc's
            original code :-)

The documentation also states that:
Note that, since chunks are adjacent to each other in memory, if you know the
address of the first chunk (lowest address) in a heap, you can iterate through
all the chunks in the heap by using the size information, but only by
increasing address, although it may be difficult to detect when you've hit the
last chunk in the heap.

Remarks:
    + Note that the bytes are little-endian in the heap dump file.
    + Within the malloc library, a "chunk pointer" or mchunkptr does not
    point to the beginning of the chunk, but to the last word in the
    previous chunk - i.e. the first field in mchunkptr is not valid
    unless you know the previous chunk is free.
"""

from enum import Enum
import math
import numpy as np
from tqdm import tqdm
import os

from utils.debugging import dp, get_now_str
from utils.entropy import get_entropy
from utils.file_handling import delete_json_and_raw_file, get_all_nested_files
from utils.heap_dump import convert_block_index_to_address, convert_int_address_to_block_index, get_blocks_from_heap_dump, get_heap_size_in_bytes, is_address_in_heap_dump, is_valid_pointer
from utils.json_annotation import get_heap_start_addr, get_json_annotations, get_keys_addresses
from utils.mem_utils import block_bytes_to_int, hex_str_to_addr, int_to_little_endian_hex_string, is_8_bytes_aligned

INPUT_RAW_HEAP_DUMP_FILE_PATH = "/home/onyr/code/phdtrack/phdtrack_data_clean/Training/Training/basic/V_7_1_P1/24/17016-1643962152-heap.raw"
KEY_MIN_BYTE_SIZE = 24

# -------------------- CLI arguments -------------------- #
import sys
import argparse

# wrapped program flags
class CLIArguments:
    args: argparse.Namespace

    def __init__(self) -> None:
        self.__log_raw_argv()
        self.__parse_argv()
    
    def __log_raw_argv(self) -> None:
        print("Passed program params:")
        for i in range(len(sys.argv)):
            print("param[{0}]: {1}".format(
                i, sys.argv[i]
            ))
    
    def __parse_argv(self) -> None:
        """
        python main [ARGUMENTS ...]
        """
        parser = argparse.ArgumentParser(description='Program [ARGUMENTS]')
        parser.add_argument(
            "--delete",
            action='store_true',
            help="Delete all annotation files and their corresponding heap dump files, when the chunk parsing is broken."
        )
        parser.add_argument(
            '--debug', 
            action='store_true',
            help="debug, True or False"
        )
        # add file path or directory path argument
        parser.add_argument(
            '--input',
            type=str,
            help="Input as file path or directory path"
        )

        # save parsed arguments
        self.args = parser.parse_args()

        # overwrite debug flag
        os.environ["DEBUG"] = "True" if self.args.debug else "False"

        # log parsed arguments
        print("Parsed program params:")
        for arg in vars(self.args):
            print("{0}: {1}".format(
                arg, getattr(self.args, arg)
            ))


class MallocHeaderFlags:
    """
    The flags of the malloc header (m-header).

    P: Previous chunk is in use (allocated by application)
    M: This chunk was allocated using mmap
    A: The main arena uses the application's heap
    """
    def __init__(
            self,
            a: bool,
            m: bool,
            p: bool
        ):
        self.a = a
        self.m = m
        self.p = p

    def __str__(self):
        return f"MallocHeaderFlags(a={self.a}, m={self.m}, p={self.p})"

    def __repr__(self):
        return str(self)

class MallocHeader:
    """
    The malloc header (m-header) which contains size and flags.
    """
    def __init__(
            self,
            block_index: int,
            size: int,
            flags: MallocHeaderFlags
        ):
        self.block_index = block_index
        self.size = size
        self.flags = flags

    def __str__(self):
        return f"MallocHeader(block_index={self.block_index}, size={self.size}, flags={self.flags})"

    def __repr__(self):
        return str(self)

class Chunk:
    """
    A chunk of memory.
    """
    def __init__(
            self,
            block_index: int,
            address: int,
            size: int,
            flags: MallocHeaderFlags,
        ):
        self.user_start_block_index = block_index # index of first block of the user data
        self.size = size # size of the user data in bytes
        self.flags = flags # flags of the malloc header
        self.address = address # address of first block of the user data
        self.mchunkptr = address - 2 * 8 # address of the mchunkptr

        # annotations and statistics
        self.is_free = False
        self.annotations: list[ChunkAnnotation] = []
        self.first_bytes_entropy = -math.inf

    def __str__(self):
        flags_as_str = f"[A={self.flags.a}, M={self.flags.m}, P={self.flags.p}]"
        return f"Chunk(block_index={self.user_start_block_index}, size={self.size}, flags={flags_as_str})"

    def __repr__(self):
        return str(self)

def check_first_block_is_only_zeros(blocks: np.ndarray) -> bool:
    """
    Check that the first block is only composed of zeros.
    """
    first_block = blocks[0]
    if first_block.sum() == 0:
        return True
    return False

def parse_size(block: np.ndarray) -> int:
    """
    Parse the size of a block.
    """
    size_and_flags = int.from_bytes(block, byteorder='little')
    size = size_and_flags & ~0x07  # Clear the last 3 bits to get the size
    return size

def parse_malloc_header(block: np.ndarray) -> MallocHeader:
    """
    Parse the malloc header from a block.
    """
    # parse size
    size = parse_size(block)
    
    # parse flags
    size_and_flags = int.from_bytes(block, byteorder='little')
    flags = MallocHeaderFlags(
        a=bool(size_and_flags & 0x04),
        m=bool(size_and_flags & 0x02),
        p=bool(size_and_flags & 0x01)
    )
    return MallocHeader(len(block), size, flags)

def check_chunk_has_only_zeros(
        blocks: np.ndarray,
        chunk: Chunk,
    ) -> bool:
    """
    Check that a chunk is only composed of zeros.
    WARN: On a free block, there can (should?) be a footer.
    """
    if chunk.is_free:
        range_end = chunk.user_start_block_index + (chunk.size // 8) - 1
    else:
        range_end = chunk.user_start_block_index + (chunk.size // 8)
    
    for i in range(chunk.user_start_block_index, range_end):
        if i >= len(blocks):
            dp(
                f"WARN: Chunk [{chunk.address}] {chunk} is out of bounds. "
                f"Last block index: {len(blocks) - 1} "
                f"Iteration index: {i} "
                )

        else:
            if blocks[i].sum() != 0:
                return False
        
    return True

def print_chunk(blocks: np.ndarray, chunk: Chunk):
    """
    Print a chunk.
    """
    if chunk.is_free:
        chunk_status = "free"
    else:
        chunk_status = "in-use"
    if is_chunk_footer_value_correct(blocks, chunk):
        footer_status = "correct"
    else:
        footer_status = "incorrect"
    dp(f"Printing Chunk [addr:{int_to_little_endian_hex_string(chunk.address)}] [status:{chunk_status}] [footer:{footer_status}] {chunk}")
    for i in range(chunk.user_start_block_index - 1, chunk.user_start_block_index + (chunk.size // 8) - 1):
        if i >= len(blocks) or i < 0:
            dp(
                f"WARN: Chunk [{chunk.address}] {chunk} is out of bounds. "
                f"Last block index: {len(blocks) - 1} "
                f"Iteration index: {i} "
                )
        else:
            block = blocks[i].tobytes()
            block_as_int = block_bytes_to_int(block)
            tail_info = ""
            if i == chunk.user_start_block_index - 1:
                tail_info = "-malloc-header-"
            elif i == chunk.user_start_block_index + (chunk.size // 8) - 2:
                tail_info = "-footer-"
            dp(f"Block [{i}]: \t {blocks[i].tobytes()} \t\t {block_as_int} \t\t {tail_info}")

def is_free_chunk(
        blocks: np.ndarray,
        chunks: list[Chunk],
        current_chunk_index: int,
    ) -> bool:
    """
    Determine if a chunk is free.
    """
    chunk = chunks[current_chunk_index]

    # # Forward and backward pointers are the next two 8-byte
    # # fields after the header.
    # forward_pointer_block = blocks[chunk.user_start_block_index].tobytes()
    # if not is_valid_pointer(
    #         forward_pointer_block,
    #         heap_start_addr,
    #         heap_size_in_bytes,
    #     ):
    #     return False

    # backward_pointer_block = blocks[chunk.user_start_block_index + 1].tobytes()
    # if not is_valid_pointer(
    #         backward_pointer_block,
    #         heap_start_addr,
    #         heap_size_in_bytes,
    #     ):
    #     return False

    # WARN: Those pointers pointing to chunks are pointing to the
    # mchunkptr !!!

    # # check the forward pointer points to the next chunk
    # # NOTE: don't check if last chunk
    # if current_chunk_index != len(chunks) - 1:
    #     next_chunk = chunks[current_chunk_index + 1]
    #     forward_pointer_addr = block_bytes_to_addr(forward_pointer_block)
    #     if forward_pointer_addr < next_chunk.mchunkptr or forward_pointer_addr > next_chunk.mchunkptr + next_chunk.size + 2:
    #         return False

    # # check the backward pointer points to the previous chunk
    # previous_chunk = chunks[current_chunk_index - 1]
    # backward_pointer_addr = block_bytes_to_addr(backward_pointer_block)
    # if backward_pointer_addr < previous_chunk.mchunkptr or backward_pointer_addr > previous_chunk.mchunkptr + previous_chunk.size + 2:
    #     return False

    # check if P flag of the next chunk is false
    # this means that this chunk is not in use (free)
    if current_chunk_index != len(chunks) - 1:
        next_chunk = chunks[current_chunk_index + 1]
        if next_chunk.flags.p == True:
            return False # current chunk is not free
    else:
        # last chunk, check if zero chunk
        if not check_chunk_has_only_zeros(blocks, chunk):
            return False
        
    # # check that the blocks after pointer blocks are only composed of zeros (except footer)
    # for i in range(chunk.user_start_block_index + 2, chunk.user_start_block_index + (chunk.size // 8) - 1): # -1 since last block is footer size
    #     if i >= len(blocks):
    #         print(
    #             f"WARN: Chunk [{chunk.address}] {chunk} is out of bounds. "
    #             f"Last block index: {len(blocks) - 1} "
    #             f"Iteration index: {i} "
    #             )
    #     else:
    #         if blocks[i].sum() != 0 or (
    #             i < chunk.user_start_block_index + 4 and 
    #             is_valid_pointer(
    #                 blocks[i].tobytes(),
    #                 heap_start_addr,
    #                 heap_size_in_bytes,
    #             )
    #         ):
    #             return False

    return True

class ChunkAnnotation(Enum):
    ChunkContainsKey = 1
    ChunkContainsSSHStruct = 2
    ChunkContainsSessionState = 3

def get_addresses_of_annotations(
        json_annotations: dict,
        blocks: np.ndarray,
        heap_start_addr: int,
        heap_size_in_bytes: int,
    ):
    """
    Get the addresses of the annotations. 
        + Keys addresses
        + SSH_STRUCT_ADDR
        + SESSION_STATE_ADDR
    """
    # get all annotation addresses
    keys_addresses, key_address_to_name = get_keys_addresses(json_annotations)
    ssh_struct_addr = hex_str_to_addr(json_annotations["SSH_STRUCT_ADDR"])
    session_state_addr = hex_str_to_addr(json_annotations["SESSION_STATE_ADDR"])

    # assert that all addresses of annotations are in the heap dump
    assert is_address_in_heap_dump(
        ssh_struct_addr,
        heap_start_addr,
        heap_size_in_bytes,
    ), "SSH_STRUCT_ADDR is not in the heap dump."
    assert is_address_in_heap_dump(
        session_state_addr,
        heap_start_addr,
        heap_size_in_bytes,
    ), "SESSION_STATE_ADDR is not in the heap dump."
    for key_addr in keys_addresses:
        assert is_address_in_heap_dump(
            key_addr,
            heap_start_addr,
            heap_size_in_bytes,
        ), f"KEY_ADDR {key_addr} for key {key_address_to_name[key_addr]} is not in the heap dump."

    # Key checkings
    # assert that the block at a given key address is the first block of the key value
    for key_addr, key_name in key_address_to_name.items():
        key_hex_string = json_annotations[key_name]
        key_bytes = bytes.fromhex(key_hex_string) # big endian in JSON

        #### first check on the first block
        # get the 8 first bytes
        key_8_first_bytes = key_bytes[:8]

        block_bytes_at_key_addr = blocks[convert_int_address_to_block_index(key_addr, heap_start_addr)].tobytes()
        
        # perform check for first block
        assert key_8_first_bytes == block_bytes_at_key_addr, (
            f"Key [{key_name}] at address [{key_addr}] does not start at the first block of the key value. "
            f"Expected (JSON): {key_8_first_bytes} "
            f"Actual (Heap): {block_bytes_at_key_addr}"
        )

        ### do another full key value check
        key_byte_size = int(json_annotations[f"{key_name}_LEN"])
        # round to upper 8 bytes
        nb_blocks_for_key = (key_byte_size + 7) // 8
        # append bytes of the blocks
        accumulated_key_bytes = b""
        for i in range(0,  nb_blocks_for_key):
            block_bytes = blocks[convert_int_address_to_block_index(key_addr + i * 8, heap_start_addr)].tobytes()
            accumulated_key_bytes += block_bytes
        
        # cut the key bytes to the actual key size
        key_bytes = key_bytes[:key_byte_size]

        # check that the key bytes are the same
        assert key_bytes == accumulated_key_bytes, (
            f"Key [{key_name}] at address [{key_addr}] does not have the same bytes as the key value. "
            f"Expected (JSON): {key_bytes} "
            f"Actual (Heap): {accumulated_key_bytes}"
        )




    return keys_addresses, ssh_struct_addr, session_state_addr
    
def annotate_chunk(
        chunk: Chunk,
        keys_addresses: list[int],
        ssh_struct_addr: int,
        session_state_addr: int,
    ):
    """
    Annotate a chunk with the corresponding annotations.
    Annotations are from JSON annotation file.

    NOTE: Annotations should be done after free chunk detection.
    """
    
    def check_chunk_is_not_free_then_annotate(chunk: Chunk, annotation: ChunkAnnotation):
        """
        Check that a chunk is not free.
        Then annotate the chunk with the annotation.
        """
        assert not chunk.is_free, (
            f"Chunk [{chunk.address}] {chunk} is free. "
            f"Cannot annotate a free chunk with {annotation}. "
        )
        chunk.annotations.append(annotation)

    # annotate chunk
    if chunk.address in keys_addresses:
        check_chunk_is_not_free_then_annotate(
            chunk,
            ChunkAnnotation.ChunkContainsKey
        )
    elif chunk.address == ssh_struct_addr:
        check_chunk_is_not_free_then_annotate(
            chunk,
            ChunkAnnotation.ChunkContainsSSHStruct
        )
    elif chunk.address == session_state_addr:
        check_chunk_is_not_free_then_annotate(
            chunk,
            ChunkAnnotation.ChunkContainsSessionState
        )

def is_chunk_footer_value_correct(
        blocks: np.ndarray,
        chunk: Chunk,
    ) -> bool:
    """
    The footer of the chunk contains the size of the chunk.
    """
    footer_block_index = chunk.user_start_block_index + (chunk.size // 8) - 2
    # check that footer block is in bounds
    if footer_block_index >= len(blocks):
        dp(f"Footer block is out of bounds for chunk: {chunk}")
        return False

    footer_block = blocks[footer_block_index]
    footer_size = parse_size(footer_block)

    if footer_size != chunk.size:
        return False
    
    return True

def is_chunk_footer_an_annotation_address(
        blocks: np.ndarray,
        chunk: Chunk,
        heap_start_addr: int,
        keys_addresses: list[int],
        ssh_struct_addr: int,
        session_state_addr: int,
    ) -> bool:
    """
    Check that the footer of a chunk is not an annotation address.
    """
    footer_block_index = chunk.user_start_block_index + (chunk.size // 8) - 2

    # check that footer block is in bounds
    if footer_block_index >= len(blocks):
        dp("Footer block is out of bounds for chunk: {chunk}")
        return False
    
    footer_addr = convert_block_index_to_address(footer_block_index, heap_start_addr)
    
    if footer_addr in keys_addresses:
        return True
    elif footer_addr == ssh_struct_addr:
        return True
    elif footer_addr == session_state_addr:
        return True
    
    return False

def compute_entropy_of_chunk_user_data(
        blocks: np.ndarray,
        chunk: Chunk,
        heap_start_addr: int,
    ):
    """
    Compute the entropy of the first bytes of the user data of a chunk.
    """
    # round to upper 8 bytes
    nb_blocks_for_key = (KEY_MIN_BYTE_SIZE + 7) // 8
    # append bytes of the blocks
    accumulated_bytes = b""
    for i in range(0,  nb_blocks_for_key):
        block_bytes = blocks[
            convert_int_address_to_block_index(
                chunk.address + i * 8, 
                heap_start_addr)
            ].tobytes()
        accumulated_bytes += block_bytes
    
    # cut the key bytes to the actual key size
    cropped_first_bytes = accumulated_bytes[:KEY_MIN_BYTE_SIZE]

    # compute entropy
    entropy = get_entropy(cropped_first_bytes)
    return entropy

def parsing_blocks_to_chunks_with_stats(
        blocks: np.ndarray,
        heap_start_addr: int,
        heap_size_in_bytes: int,
        json_annotations: dict,
        raw_file_path: str,
    ):
    """
    Parse the blocks of the heap dump to chunks.
    """
    chunks = []

    # Initialize counters
    nb_chunks = 0
    nb_previous_chunk_in_use = 0
    nb_nmap_chunks = 0
    nb_main_arena_chunks = 0
    nb_potential_footers_with_annotations = 0

    # Start parsing from the second block
    i = 1
    while i < len(blocks):

        # DEBUG
        if os.environ["DEBUG"] == "True" and i < 130:
            block_as_int = block_bytes_to_int(blocks[i].tobytes())
            dp(f"i: {i}  Block [{i}]: \t {blocks[i].tobytes()} \t\t {block_as_int}")

        # Parse the malloc header
        malloc_header = parse_malloc_header(blocks[i])

        # WARN: if chunk size is 0, it means that there is a problem
        # with the parsing of the malloc header.
        # This can happen if the heap dump file is corrupted.
        # In this case, log error and skip this file entirely.
        if malloc_header.size == 0:
            print(
                f"ERROR: malloc_header.size is 0. "
                f"Skipping file: {raw_file_path}"
            )
            return None

        # Create a chunk object
        chunk = Chunk(
            i+1,
            convert_block_index_to_address(i+1, heap_start_addr),
            malloc_header.size,
            malloc_header.flags
        )

        # Update statistics
        nb_chunks += 1
        if malloc_header.flags.a:
            nb_main_arena_chunks += 1
        if malloc_header.flags.m:
            nb_nmap_chunks += 1
        if malloc_header.flags.p:
            nb_previous_chunk_in_use += 1

        if nb_chunks < 10 or nb_chunks > 900:
            dp(f"Parsed Chunk [{nb_chunks}]: {chunk}")

        # save chunk
        chunks.append(chunk)

        # Move to the next chunk
        i += (malloc_header.size // 8)


    # load annotations
    try:
        keys_addresses, ssh_struct_addr, session_state_addr = get_addresses_of_annotations(
            json_annotations,
            blocks,
            heap_start_addr,
            heap_size_in_bytes,
        )
    except Exception as err:
        print(f"ERROR: {err}", f"Skipping file: {raw_file_path}")
        return None
        
    # chunk post processing
    for i in range(0, len(chunks)):
        chunk = chunks[i]

        # free chunk detection
        if is_free_chunk(blocks, chunks, i):
            chunk.is_free = True

        # add annotations AFTER free chunk detection
        annotate_chunk(
            chunk,
            keys_addresses,
            ssh_struct_addr,
            session_state_addr,
        )

        # check if chunk footer is an annotation address
        if is_chunk_footer_an_annotation_address(
                blocks,
                chunk,
                heap_start_addr,
                keys_addresses,
                ssh_struct_addr,
                session_state_addr,
            ):
            dp(f"WARN: Chunk [{chunks[i].user_start_block_index}] footer is an annotation address: {chunks[i]}")
            nb_potential_footers_with_annotations += 1
        
        # compute entropy of first 12 bytes of chunk user data
        # if chunk is not free
        if not chunk.is_free:
            chunk.first_bytes_entropy = compute_entropy_of_chunk_user_data(
                blocks, chunk, heap_start_addr
            )


    # statistics
    stats = {}
    stats["raw_file_path"] = raw_file_path
    stats["nb_chunks"] = nb_chunks
    stats["nb_blocks"] = len(blocks)
    stats["nb_previous_chunk_in_use"] = nb_previous_chunk_in_use
    stats["nb_nmap_chunks"] = nb_nmap_chunks
    stats["nb_main_arena_chunks"] = nb_main_arena_chunks
    stats["nb_potential_footers_with_annotations"] = nb_potential_footers_with_annotations

    return chunks, stats

def pipeline(raw_file_path: str, cli: CLIArguments):

    # print date, time and file path
    dp(f"Processing file: {raw_file_path}")
    dp(f"Datetime: {get_now_str()}")

    # get json annotations
    json_file_path = raw_file_path.replace("-heap.raw", ".json")
    json_annotations = get_json_annotations(json_file_path)

    # load the heap dump blocks
    blocks = get_blocks_from_heap_dump(
        raw_file_path
    )

    # determine the heap start address and the heap size in bytes
    heap_start_addr = get_heap_start_addr(json_annotations)
    heap_size_in_bytes = get_heap_size_in_bytes(blocks)

    # the heap dump starts with an allocated block
    # the first block is only composed of zeros
    # the second block is the malloc header (m-header)
    # which contains size and flags of the first allocated block
    assert check_first_block_is_only_zeros(blocks), (
        "The first block is not composed of only zeros."
    )

    # parse the blocks to chunks
    parsing_res = parsing_blocks_to_chunks_with_stats(
        blocks,
        heap_start_addr,
        heap_size_in_bytes,
        json_annotations,
        raw_file_path,
    )
    if parsing_res is None:
        if cli.args.delete:
            delete_json_and_raw_file(raw_file_path)
        return None
    chunks, stats = parsing_res # unpack parsing results

    # counters
    nb_free_chunks = 0
    nb_zeros_chunks = 0
    nb_correct_footer_chunks = 0
    nb_chunk_both_free_and_correct_footer = 0
    nb_chunks_free_and_annotated = 0
    nb_chunks_in_use_correct_footer_annotated = 0
    nb_chunks_in_use_correct_footer_annotated_and_key = 0

    sizes_key_chunks: list[int] = []

    nb_annotated_chunks = 0
    for i in range(0, len(chunks)):
        chunk = chunks[i]

        # count free chunks
        if chunk.is_free:
            nb_free_chunks += 1
            
            if nb_free_chunks < 10:
                dp(f"Free Chunk [addr:{int_to_little_endian_hex_string(chunk.address)}]: {chunks[i]}")

            # check that a free chunk is not annotated
            if len(chunks[i].annotations) > 0:
                nb_chunks_free_and_annotated += 1
                print(f"WARN: Chunk [{chunk.user_start_block_index}] is free but also annotated: {chunks[i]}")

        # check if chunk is only composed of zeros            
        if check_chunk_has_only_zeros(blocks, chunks[i]):
            dp(f"Chunk [{chunk.user_start_block_index}] is only composed of zeros: {chunks[i]}")
            nb_zeros_chunks += 1
        
        # check if chunk footer value is correct
        if is_chunk_footer_value_correct(blocks, chunks[i]):
            nb_correct_footer_chunks += 1

            # count free and correct footer chunks
            if chunk.is_free:
                nb_chunk_both_free_and_correct_footer += 1
            else:
                if len(chunk.annotations) > 0:
                    nb_chunks_in_use_correct_footer_annotated += 1
                    if ChunkAnnotation.ChunkContainsKey in chunk.annotations:
                        nb_chunks_in_use_correct_footer_annotated_and_key += 1
        
        # count annotated chunks
        if len(chunk.annotations) > 0:
            nb_annotated_chunks += 1
        
        # count key chunks
        if ChunkAnnotation.ChunkContainsKey in chunk.annotations:
            sizes_key_chunks.append(chunk.size)
        
    if os.environ["DEBUG"] == "True":
        # print chunks with annotations
        for i in range(0, len(chunks)):
            chunk = chunks[i]
            if len(chunk.annotations) > 0:
                dp(f"Chunk [{int_to_little_endian_hex_string(chunk.address)}] {chunk} has annotations: {chunk.annotations}")

        # print content of chunks
        #dp("Content of chunks:")
        #dp_chunk(blocks, chunks[-1])
        for i in range(0, len(chunks)):
            if chunks[i].user_start_block_index == 80 or chunks[i].user_start_block_index == 8180:
                print_chunk(blocks, chunks[i])
    
    # compute percentage of free chunks
    percentage_free_chunks = nb_free_chunks / len(chunks) * 100
    dp(f"Percentage of free chunks: {percentage_free_chunks}%")

    # compute percentage of blocks in free chunks
    nb_blocks_in_free_chunks = 0
    for i in range(0, len(chunks)):
        if chunks[i].is_free:
            nb_blocks_in_free_chunks += chunks[i].size // 8
    percentage_blocks_in_free_chunks = nb_blocks_in_free_chunks / len(blocks) * 100
    dp(f"Percentage of blocks in free chunks: {percentage_blocks_in_free_chunks}%")

    # check that the number of annotated chunks is 8
    # 6 keys + 1 SSH_STRUCT_ADDR + 1 SESSION_STATE_ADDR
    if nb_annotated_chunks != 8:
        dp(f"ERROR: nb_annotated_chunks != 8: {nb_annotated_chunks} for file {raw_file_path}")
        raise Exception(f"nb_annotated_chunks != 8: {nb_annotated_chunks} for file {raw_file_path}")

    # sort chunks by first bytes entropy
    chunks.sort(key=lambda x: x.first_bytes_entropy, reverse=True)

    # count number of chunks with max entropy
    max_entropy = chunks[0].first_bytes_entropy
    nb_chunks_with_max_entropy = 0
    for i in range(0, len(chunks)):
        if chunks[i].first_bytes_entropy == max_entropy:
            nb_chunks_with_max_entropy += 1
        else:
            break
    
    # print chunks with max entropy, with their entropy value and annotations
    dp(f"Chunks with max entropy ({max_entropy}):")
    nb_chunks_with_max_entropy_and_annotated = 0
    nb_chunks_with_max_entropy_and_key_annotation = 0
    for i in range(0, nb_chunks_with_max_entropy):
        chunk: Chunk = chunks[i]
        dp(f"-> [e:{chunks[i].first_bytes_entropy}] [first_block:{blocks[chunk.user_start_block_index]}] Chunk [{int_to_little_endian_hex_string(chunk.address)}] size: {chunk.size}, annotations: {chunk.annotations}")

        if len(chunk.annotations) > 0:
            nb_chunks_with_max_entropy_and_annotated += 1
            if ChunkAnnotation.ChunkContainsKey in chunk.annotations:
                nb_chunks_with_max_entropy_and_key_annotation += 1
            

    dp(f"number of chunks with max entropy: {nb_chunks_with_max_entropy}")
    dp(f"number of chunks with max entropy and annotations: {nb_chunks_with_max_entropy_and_annotated}")
    dp(f"number of chunks with max entropy and key annotation: {nb_chunks_with_max_entropy_and_key_annotation}")

    # print the entropy of all the chunks with key annotation
    if cli.args.debug:
        dp("Entropy of chunks with key annotation:")
        for i in range(0, len(chunks)):
            chunk: Chunk = chunks[i]
            if ChunkAnnotation.ChunkContainsKey in chunk.annotations:
                dp(f"-> [e:{chunks[i].first_bytes_entropy}] [first_block:{blocks[chunk.user_start_block_index]}] Chunk [{int_to_little_endian_hex_string(chunk.address)}] size: {chunk.size}, annotations: {chunk.annotations}")

    # print statistics
    dp(f"number of chunks: {stats['nb_chunks']}")
    dp(f"number of chunks with P=1: {stats['nb_previous_chunk_in_use']}")
    dp(f"number of chunks with M=1: {stats['nb_nmap_chunks']}")
    dp(f"number of chunks with A=1: {stats['nb_main_arena_chunks']}")
    dp(f"number of free chunks: {nb_free_chunks}")
    dp(f"number of chunks only composed of zeros: {nb_zeros_chunks}")
    dp(f"number of blocks in free chunks: {nb_blocks_in_free_chunks}")
    dp(f"number of chunks with correct footer: {nb_correct_footer_chunks}")
    dp(f"number of chunks both free and with correct footer: {nb_chunk_both_free_and_correct_footer}")
    dp(f"number of chunks free and annotated: {nb_chunks_free_and_annotated}")
    dp(f"number of potential footers with annotations: {stats['nb_potential_footers_with_annotations']}")
    dp(f"number of annotated chunks: {nb_annotated_chunks}")
    dp(f"number of chunks in used, with correct footer, and annotated: {nb_chunks_in_use_correct_footer_annotated}")

    # save statistics
    stats["nb_free_chunks"] = nb_free_chunks
    stats["nb_zeros_chunks"] = nb_zeros_chunks
    stats["nb_blocks_in_free_chunks"] = nb_blocks_in_free_chunks
    stats["correct_footer_chunks"] = nb_correct_footer_chunks
    stats["nb_chunk_both_free_and_correct_footer"] = nb_chunk_both_free_and_correct_footer
    stats["nb_chunks_free_and_annotated"] = nb_chunks_free_and_annotated
    stats["nb_annotated_chunks"] = nb_annotated_chunks
    stats["nb_chunks_in_use_correct_footer_annotated"] = nb_chunks_in_use_correct_footer_annotated
    stats["nb_chunks_in_use_correct_footer_annotated_and_key"] = nb_chunks_in_use_correct_footer_annotated_and_key
    
    # save lists
    stats["sizes_key_chunks"] = sizes_key_chunks

    # delete stuff to free memory
    del blocks
    del chunks

    return stats

def main():
    """
    Main function.
    """
    cli = CLIArguments()

    all_sizes_key_chunks: list[int] = []

    global_stats = {}
    global_stats["nb_parsed_files"] = 0
    global_stats["nb_chunks"] = 0
    global_stats["nb_blocks"] = 0
    global_stats["nb_previous_chunk_in_use"] = 0
    global_stats["nb_nmap_chunks"] = 0
    global_stats["nb_main_arena_chunks"] = 0
    global_stats["nb_free_chunks"] = 0
    global_stats["nb_zeros_chunks"] = 0
    global_stats["nb_blocks_in_free_chunks"] = 0
    global_stats["nb_correct_footer_chunks"] = 0
    global_stats["nb_chunk_both_free_and_correct_footer"] = 0
    global_stats["nb_chunks_free_and_annotated"] = 0
    global_stats["nb_potential_footers_with_annotations"] = 0
    global_stats["nb_deleted_raw_files"] = 0
    global_stats["nb_annoted_chunks"] = 0
    global_stats["nb_chunks_in_use_correct_footer_annotated"] = 0
    global_stats["nb_chunks_in_use_correct_footer_annotated_and_key"] = 0

    global_stats["nb_skipped_files"] = 0

    def store_global_stats(stats: dict | None):
        if stats is None:
            global_stats["nb_skipped_files"] += 1
            if cli.args.delete:
                global_stats["nb_deleted_raw_files"] += 1
            return
        
        global_stats["nb_parsed_files"] += 1
        global_stats["nb_chunks"] += stats["nb_chunks"]
        global_stats["nb_blocks"] += stats["nb_blocks"]
        global_stats["nb_previous_chunk_in_use"] += stats["nb_previous_chunk_in_use"]
        global_stats["nb_nmap_chunks"] += stats["nb_nmap_chunks"]
        global_stats["nb_main_arena_chunks"] += stats["nb_main_arena_chunks"]
        global_stats["nb_free_chunks"] += stats["nb_free_chunks"]
        global_stats["nb_zeros_chunks"] += stats["nb_zeros_chunks"]
        global_stats["nb_blocks_in_free_chunks"] += stats["nb_blocks_in_free_chunks"]
        global_stats["nb_correct_footer_chunks"] += stats["correct_footer_chunks"]
        global_stats["nb_chunk_both_free_and_correct_footer"] += stats["nb_chunk_both_free_and_correct_footer"]
        global_stats["nb_chunks_free_and_annotated"] += stats["nb_chunks_free_and_annotated"]
        global_stats["nb_potential_footers_with_annotations"] += stats["nb_potential_footers_with_annotations"]
        global_stats["nb_annoted_chunks"] += stats["nb_annotated_chunks"]
        global_stats["nb_chunks_in_use_correct_footer_annotated"] += stats["nb_chunks_in_use_correct_footer_annotated"]
        global_stats["nb_chunks_in_use_correct_footer_annotated_and_key"] += stats["nb_chunks_in_use_correct_footer_annotated_and_key"]

        all_sizes_key_chunks.extend(stats["sizes_key_chunks"])

    if cli.args.input is None:
        # default input
        store_global_stats(pipeline(INPUT_RAW_HEAP_DUMP_FILE_PATH, cli))
    else:
        if cli.args.input.endswith("-heap.raw"):
            # input is single file
            store_global_stats(pipeline(cli.args.input, cli))
        else:
            # input is directory
            print(f"Input is directory: {cli.args.input}")
            raw_file_paths = get_all_nested_files(cli.args.input, "-heap.raw")
            if len(raw_file_paths) < 1:
                print(f"No raw heap dump file found in directory {cli.args.input}")
                exit(1)
            
            # Initialize the tqdm object
            with tqdm(total=len(raw_file_paths), desc="Processing files") as pbar:
                for raw_file_path in raw_file_paths:
                    # Update the postfix data (this will display alongside the progress bar)
                    pbar.set_postfix(
                        file=raw_file_path.split('/')[-1].replace("-heap.raw", ""), 
                        refresh=True
                    )

                    # Execute the pipeline and store statistics
                    store_global_stats(pipeline(raw_file_path, cli))

                    # Update the progress bar
                    pbar.update(1)
            
    # print statistics
    print("------> Statistics:")
    print(f"Total number of parsed files: {global_stats['nb_parsed_files']}")
    
    if global_stats['nb_parsed_files'] == 1:
        print(f"File path: {INPUT_RAW_HEAP_DUMP_FILE_PATH}")
    
    print(f"Total number of skipped files: {global_stats['nb_skipped_files']}")
    
    print(f"Total number of chunks: {global_stats['nb_chunks']}")
    print(f"Total number of blocks: {global_stats['nb_blocks']}")
    print(f"Total number of chunks with P=1: {global_stats['nb_previous_chunk_in_use']}")
    print(f"Total number of chunks with M=1: {global_stats['nb_nmap_chunks']}")
    print(f"Total number of chunks with A=1: {global_stats['nb_main_arena_chunks']}")
    print(f"Total number of free chunks: {global_stats['nb_free_chunks']}")
    print(f"Total number of chunks only composed of zeros: {global_stats['nb_zeros_chunks']}")
    print(f"Total number of blocks in free chunks: {global_stats['nb_blocks_in_free_chunks']}")
    print(f"Total number of chunks with correct footer value: {global_stats['nb_correct_footer_chunks']}")
    print(f"Total number of chunks both free and with correct footer value: {global_stats['nb_chunk_both_free_and_correct_footer']}")
    print(f"Total number of chunks free and annotated: {global_stats['nb_chunks_free_and_annotated']}")
    print(f"Total number of potential footers with annotations (should be 0): {global_stats['nb_potential_footers_with_annotations']}")
    print(f"Total number of annotated chunks: {global_stats['nb_annoted_chunks']}")
    print(f"Total number of chunks in used, with correct footer, and annotated: {global_stats['nb_chunks_in_use_correct_footer_annotated']}")
    print(f"Total number of chunks in used, with correct footer, and key annotated: {global_stats['nb_chunks_in_use_correct_footer_annotated_and_key']}")

    if cli.args.delete:
        print(f"Total number of deleted raw files: {global_stats['nb_deleted_raw_files']}")

    # compute percentage of free chunks
    percentage_free_chunks = global_stats["nb_free_chunks"] / global_stats["nb_chunks"] * 100
    print(f"Percentage of free chunks: {percentage_free_chunks}%")

    # compute percentage of blocks in free chunks
    percentage_blocks_in_free_chunks = global_stats["nb_blocks_in_free_chunks"] / global_stats["nb_blocks"] * 100
    print(f"Percentage of blocks in free chunks: {percentage_blocks_in_free_chunks}%")

    # compute percentage of free chunks with correct footer value
    percentage_free_chunks_with_correct_footer = global_stats["nb_chunk_both_free_and_correct_footer"] / global_stats["nb_free_chunks"] * 100
    print(f"Percentage of free chunks with correct footer value: {percentage_free_chunks_with_correct_footer}%")
    
    # computer percentage of in-use chunks with correct footer value
    percentage_in_use_chunks_with_correct_footer = (global_stats["nb_correct_footer_chunks"] - global_stats["nb_chunk_both_free_and_correct_footer"]) / (global_stats["nb_chunks"] - global_stats["nb_free_chunks"]) * 100
    print(f"Percentage of in-use chunks with correct footer value: {percentage_in_use_chunks_with_correct_footer}%")

    # compute the average number of annoted chunks per file
    average_nb_annoted_chunks_per_file = global_stats["nb_annoted_chunks"] / global_stats["nb_parsed_files"]
    print(f"Average number of annoted chunks per file: {average_nb_annoted_chunks_per_file}")

    # compute the average number of chunks in use with correct footer and annotated per file
    average_nb_chunks_in_use_correct_footer_annotated_per_file = global_stats["nb_chunks_in_use_correct_footer_annotated"] / global_stats["nb_parsed_files"]
    print(f"Average number of chunks in use with correct footer and annotated per file: {average_nb_chunks_in_use_correct_footer_annotated_per_file}")

    # compute and print the quartiles of the sizes of the key chunks
    sizes_key_chunks = np.array(all_sizes_key_chunks)
    sizes_key_chunks.sort()

    Q1_sizes_key_chunks = np.percentile(sizes_key_chunks, 25)
    Q2_sizes_key_chunks = np.percentile(sizes_key_chunks, 50)
    Q3_sizes_key_chunks = np.percentile(sizes_key_chunks, 75)

    print("Q1 size of key chunks:", Q1_sizes_key_chunks)
    print("Q2 size of key chunks:", Q2_sizes_key_chunks)
    print("Q3 size of key chunks:", Q3_sizes_key_chunks)

    # print set of sizes of key chunks
    set_sizes_key_chunks = set(sizes_key_chunks)
    print("Set of sizes of key chunks:", set_sizes_key_chunks)

    # print each unique size of key chunks, with the number of chunks with this size
    print("Sizes of key chunks with their number of occurences:")
    for size in set_sizes_key_chunks:
        print(f"Size: {size}  Number of occurences: {np.count_nonzero(sizes_key_chunks == size)}")

    # print number of sizes
    print("Number of sizes:", len(sizes_key_chunks))
    print("Number of unique sizes:", set_sizes_key_chunks)


if __name__ == "__main__":
    main()