\chapter{Conclusion}\label{chap:conclusion}

% Summarize the thesis and provide a outlook on future work.

The evolving landscape of cybersecurity necessitates robust techniques for safeguarding digital communications. OpenSSH, a pivotal element in this landscape, is a popular implementation of the Secure Shell (\acrshort{ssh}) protocol, which enables secure communication between two networked devices. The protocol is widely used in the industry, particularly in the context of remote access to servers. Using digital forensic techniques, it is possible to extract the SSH keys from memory dumps, which can then be used to decode encrypted communications thus allowing the monitoring of controlled systems. At the crux of this Masterarbeit is the development of algorithms and machine learning models to predict SSH keys within these heap dumps, focusing on using graph-like-structures and vectorization for custom embeddings. With an interdisciplinary approach that fuses traditional feature engineering with graph-based methods as well as memory modelization for inductive reasoning and learning inspired by recent developments in \acrfull{kg}s, this research not only leverages existing machine learning paradigms but also explores new avenues, such as \acrfull{gcn} applied to memory forensics. The present work also introduces a new memory forensics tool, \textit{mem2graph}, which is designed to be modular and extensible, and which can be used to generate memory graphs from memory dumps. 

\section{Summary of Results}
Below is a summary of the results achieved in the present work.

\subsection{Dataset Exploration}
A careful exploration of the dataset, and deep understanding of the original heap dumps have been invaluable in discovering patterns in the raw data. This exploration has allowed the development of a range of parsing algorithm able to extract information like structure and content of a given heap dump.

It has been discovered that the problem of finding the address of keys in the heap dump can be reduced to identifying the chunks that contain those keys. This allows to reduce the size of the problems from aroud 100 000 of blocks per heap dump, to around 1000 chunks per file. This also allows to concentrate the heap dump memory graph representation around the chunks.

It has also been demonstrated that two powerful chunk filtering techniques can drastically reduce the number of chunks to consider. The first filter criterion consist in the Shannon's entropy value of a chunk user data start bytes. This is because the keys are expected to have a high entropy compared to other raw data types. The second important criterion is the chunk byte size. It has been shown that key chunks actually have a small size in the range of possible key size. If filtering is not possible, as it is the case with \acrshort{gcn} models, those filters can actually be converted in powerful float and boolean features.

However, its worth noting that instead of doing active node filtering and data rebalancing, we have run the experiments and model training and evaluation on full memory graphs with high imbalance ratio. This is because we wanted to test GCN that are able to handle imbalanced data with graphs of varying size, and because we wanted to test the ability of the GCN to learn from the imbalanced data. 

\subsection{Memory Graph Generation}
This masterarbeit has introduced a range of algorithms able to generate memory graphs from memory dumps. The algorithms are designed to be as generic as possible, and can be applied to any memory dump dataset. The algorithms are mostly implemented in the \textit{mem2graph} program, and many exploration and sanity checking scripts are also available in Python.

With those algorithms, it is possible to parse a RAW heap dump file, and transform it into a memory graph, or memgraph. This graph is a data structure, where each node represents a memory block with a precise address in the heap. Each edge represents either a pointer pointing to another block, or materialize the fact that a block belongs to a specific chunk. In order to reduce the size of the graph, it is possible to compact the block graph into a chunk graph, where each node represents a chunk, and chunks are connected through their pointers. Those kind of graphs are only composed of Chunk Header Nodes whose address is considered the address of the related chunk. This allows to reduce the size of the graph by a factor of 10 to 100.

\subsection{Feature Engineering and Embeddings}
The memory graph can be used to extract features from the memory dump, and to apply machine learning algorithms to the memory dump. It can also be used for direct graph visualization. The memory graph serves as a direct source of embedding whether they are made manually or using readily available and tested techniques like RandomWalks or Node2Vec.

All those embeddings can be combined. The feature evaluation has shown that those features are very lowly correlated, meaning that their quality is high. However, all those different embeddings doens't have the same results on the \acrshort{ml} and \acrshort{gcn} models, depending on the strengths and weaknesses of the different models. 

However, it's worth noting that the best results are always obtained when using the Node2Vec embedding, no matter the 
type of model used. This observations are likely to be due to the fact that Node2Vec is able to capture the structure of the graph, and that the structure of the graph is the most important feature for the models, given a relatively small number of input memgraph and considering that those memgraphs are highly imbalanced.

\subsection{Conclusion on Model Performances}

In this study, we compared the efficiency of classical machine learning models and \acrfull{gcn} in the task of key chunk prediction. Our findings indicate that the choice of model largely depends on the specific metric of interest. For a balanced performance encompassing recall, precision, and AUC, the SGD Classifier from the classical models and the Very Simple GCN from the GCN models are the most promising. 

If precision is the primary metric of concern, Logistic Regression and Random Forest from the classical models excel with perfect scores, while the Very Simple GCN leads among the GCNs. For scenarios where high recall is crucial, the SGD Classifier and the Advanced GCN model stand out. Both models also perform exceptionally well in class separation, as indicated by their high AUC scores.

It's also worth noting that simpler GCN models like the Very Simple and Simple GCN tend to perform better on limited datasets, suggesting their suitability for tasks with constrained data availability. In contrast, more complex models like the Advanced GCN show promise in terms of high recall but require more extensive training data for optimal performance.

In summary, the optimal model selection is contingent upon the specific requirements of the task, whether it be balanced performance, high precision, high recall, or excellent class separation. The choice of model also depends on the availability of training data. For instance, if the dataset is limited, a simpler GCN model like the Very Simple GCN is preferable. However, if the dataset is extensive, a more complex model like the Advanced GCN is more suitable. 

\section{Outlook on Future Work}\label{conclusion:sec:future_work}

% What are the next steps? What are the open questions? What are the limitations of your work?

The current report, in conjunction with the associated Masterarbeit, has introduced numerous novel algorithms and implementations. These have been instrumental in addressing the initial research questions. However, as with most research endeavors, new queries and potential avenues for enhancement have emerged, paving the way towards further exploration.

The methodologies and algorithms introduced for the OpenSSH memory dump dataset are versatile and can be extended to other memory dump datasets utilizing the GLIBC library. Given that this library is the default for Linux, adapting the methods from this Masterarbeit to other applications requires minimal effort. The \textit{mem2graph} program is inherently modular and built for extensibility. Furthermore, this tool can be employed to produce memory graphs for diverse datasets. Thanks to the universal character of the generated embeddings and memory graphs, new datasets can be readily integrated into the \acrshort{ml} and \acrshort{dl} pipelines crafted in Python. While an extensive array of features and embedding techniques have been explored in this report, there remains ample opportunity for innovative experimentation.

For a seamless fusion of machine learning into the \textit{mem2graph} program, further effort is required. Embedding machine learning immediately post-memory graph creation can substantially boost efficiency, particularly when aiming to craft a real-time OpenSSH memory forensics utility. However, this integration is challenging due to the current limited \acrshort{ml} support within Rust.

Another avenue for enhancement involves analyzing the effects of different C libraries on allocated chunks and the layout of heap dump memory. Investigating various languages could also be insightful. Depending on the level of variation encountered, modifications to the algorithms might be required, especially concerning the architecture involved in generating or extracting heap dump configurations. Pursuing this direction could significantly advance the development of a universal machine learning-assisted memory forensics tool for key extraction.

While the background section underscores the vast array of \acrshort{ml} architectures available, it's clear that not all can be thoroughly explored. This research has primarily addressed the most common and promising ones, yet numerous others await investigation. The tools crafted to bolster \acrshort{ml} pipelines present a solid foundation for such endeavors. Another dimension to consider is hyperparameter optimization. Given the constraints of time and resources, only certain parameter ranges were tested. Expanding these tests, incorporating larger datasets, and harnessing increased computational capacity can directly enhance performance.


